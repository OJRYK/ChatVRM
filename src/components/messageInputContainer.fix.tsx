import { useState, useEffect, useCallback, useRef } from 'react'
import { MessageInput } from '@/components/messageInput'
import settingsStore from '@/features/stores/settings'
import { VoiceLanguage } from '@/features/constants/settings'
import webSocketStore from '@/features/stores/websocketStore'
import { useTranslation } from 'react-i18next'
import toastStore from '@/features/stores/toast'
import homeStore from '@/features/stores/home'
import * as sileroVad from '@/utils/sileroVad'
import { resetVadBetweenSessions, cleanupVadState } from '@/utils/vadHelper'

// AudioContext ã®å‹å®šç¾©ã‚’æ‹¡å¼µ
type AudioContextType = typeof AudioContext

// éŸ³å£°èªè­˜é–‹å§‹å¾Œã€éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œãªã„ã¾ã¾çµŒéã—ãŸå ´åˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ5ç§’ï¼‰
const INITIAL_SPEECH_TIMEOUT = 5000

// ç„¡éŸ³æ¤œå‡ºç”¨ã®çŠ¶æ…‹ã¨å¤‰æ•°ã‚’è¿½åŠ 
type Props = {
  onChatProcessStart: (text: string) => void
}

export const MessageInputContainer = ({ onChatProcessStart }: Props) => {
  const realtimeAPIMode = settingsStore((s) => s.realtimeAPIMode)
  const [userMessage, setUserMessage] = useState('')
  const [recognition, setRecognition] = useState<SpeechRecognition | null>(null)
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null)
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null)
  const keyPressStartTime = useRef<number | null>(null)
  const transcriptRef = useRef('')
  const isKeyboardTriggered = useRef(false)
  const audioBufferRef = useRef<Float32Array | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const isListeningRef = useRef(false)
  const [isListening, setIsListening] = useState(false)
  const isSpeaking = homeStore((s) => s.isSpeaking)
  const voiceOnlyMode = settingsStore((s) => s.voiceOnlyMode)
  const vadSensitivity = settingsStore((s) => s.vadSensitivity)
  const voiceSilenceMinDuration = settingsStore((s) => s.voiceSilenceMinDuration)
  const alwaysListening = settingsStore((s) => s.alwaysListening)
  const audioBufferEnabled = settingsStore((s) => s.audioBufferEnabled)
  const audioBufferDuration = settingsStore((s) => s.audioBufferDuration)
  const interruptOnSpeechDetected = settingsStore((s) => s.interruptOnSpeechDetected)
  // éŸ³å£°èªè­˜é–‹å§‹æ™‚åˆ»ã‚’ä¿æŒã™ã‚‹å¤‰æ•°ã‚’è¿½åŠ 
  const recognitionStartTimeRef = useRef<number>(0)
  // éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
  const speechDetectedRef = useRef<boolean>(false)
  // åˆæœŸéŸ³å£°æ¤œå‡ºç”¨ã®ã‚¿ã‚¤ãƒãƒ¼
  const initialSpeechCheckTimerRef = useRef<NodeJS.Timeout | null>(null)
  // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–ç”¨ã®AnalyserNode
  const analyserRef = useRef<AnalyserNode | null>(null)
  // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–ç”¨ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
  const animationFrameRef = useRef<number | null>(null)
  const selectLanguage = settingsStore((s) => s.selectLanguage)

  const { t } = useTranslation()

  // ç„¡éŸ³æ¤œå‡ºç”¨ã®è¿½åŠ å¤‰æ•°
  const lastSpeechTimestamp = useRef<number>(0)
  const silenceCheckInterval = useRef<NodeJS.Timeout | null>(null)
  const speechEndedRef = useRef<boolean>(false)
  const stopListeningRef = useRef<(() => Promise<void>) | null>(null)
  
  // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹ç„¡éŸ³æ¤œå‡ºã®ãŸã‚ã®å¤‰æ•°
  const isSpeakingRef = useRef<boolean>(false)
  const silenceStartTimeRef = useRef<number>(0)
  const audioLevelAboveThresholdRef = useRef<boolean>(false)
  
  // ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡ã®é‡è¤‡ã‚’é˜²ããƒ•ãƒ©ã‚°
  const isSubmittingRef = useRef<boolean>(false)
  
  // é–¢æ•°ã‚’useRefã§ä¿æŒã—ã¦ä¾å­˜é–¢ä¿‚ã®å¾ªç’°ã‚’é˜²ã
  const startSilenceDetectionRef = useRef<
    ((stopListeningFn: () => Promise<void>) => void) | null
  >(null)
  const clearSilenceDetectionRef = useRef<(() => void) | null>(null)
  const sendAudioBufferRef = useRef<(() => void) | null>(null)

  // éŸ³å£°åœæ­¢
  const handleStopSpeaking = useCallback(() => {
    homeStore.setState({ isSpeaking: false })
  }, [])

  // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•°
  const clearInitialSpeechCheckTimer = useCallback(() => {
    if (initialSpeechCheckTimerRef.current) {
      clearTimeout(initialSpeechCheckTimerRef.current)
      initialSpeechCheckTimerRef.current = null
    }
  }, [])

  const checkMicrophonePermission = async (): Promise<boolean> => {
    // Firefoxã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†
    if (navigator.userAgent.toLowerCase().includes('firefox')) {
      toastStore.getState().addToast({
        message: t('Toasts.FirefoxNotSupported'),
        type: 'error',
        tag: 'microphone-permission-error-firefox',
      })
      return false
    }

    try {
      // getUserMediaã‚’ç›´æ¥å‘¼ã³å‡ºã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ãƒ†ã‚£ãƒ–è¨±å¯ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’è¡¨ç¤º
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      stream.getTracks().forEach((track) => track.stop())
      return true
    } catch (error) {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æ‹’å¦ã—ãŸå ´åˆã‚„ã€ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
      console.error('Microphone permission error:', error)
      return false
    }
  }

  // getVoiceLanguageCodeã‚’useCallbackã§ãƒ©ãƒƒãƒ—ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’æ˜ç¢ºã«ã™ã‚‹
  const getVoiceLanguageCode = useCallback(
    (selectLanguage: string): VoiceLanguage => {
      switch (selectLanguage) {
        case 'ja':
          return 'ja-JP'
        case 'en':
          return 'en-US'
        case 'ko':
          return 'ko-KR'
        case 'zh':
          return 'zh-TW'
        case 'vi':
          return 'vi-VN'
        case 'fr':
          return 'fr-FR'
        case 'es':
          return 'es-ES'
        case 'pt':
          return 'pt-PT'
        case 'de':
          return 'de-DE'
        case 'ru':
          return 'ru-RU'
        case 'it':
          return 'it-IT'
        case 'ar':
          return 'ar-SA'
        case 'hi':
          return 'hi-IN'
        case 'pl':
          return 'pl-PL'
        case 'th':
          return 'th-TH'
        default:
          return 'ja-JP'
      }
    },
    []
  )

  // ç„¡éŸ³æ¤œå‡ºã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹é–¢æ•° - ä¾å­˜ãŒãªã„ã®ã§å…ˆã«å®šç¾©
  const clearSilenceDetection = useCallback(() => {
    if (silenceCheckInterval.current) {
      clearInterval(silenceCheckInterval.current)
      silenceCheckInterval.current = null
    }
  }, [])

  // clearSilenceDetectionã‚’Refã«ä¿å­˜
  useEffect(() => {
    clearSilenceDetectionRef.current = clearSilenceDetection
  }, [clearSilenceDetection])

  // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé€ä¿¡ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° - äºŒè¨€ç›®ä»¥é™ãŒé€ä¿¡ã•ã‚Œãªã„å•é¡Œã®è§£æ±º
  const sendTranscriptAsPrompt = useCallback((transcript: string) => {
    if (!transcript || transcript.trim() === '') {
      console.log('âŒ é€ä¿¡ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™');
      return false;
    }
    
    if (isSubmittingRef.current) {
      console.log('âš ï¸ æ—¢ã«é€ä¿¡å‡¦ç†ä¸­ã®ãŸã‚ã€é‡è¤‡é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™');
      return false;
    }
    
    try {
      isSubmittingRef.current = true;
      console.log(`ğŸ“¤ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé€ä¿¡ã‚’é–‹å§‹ã—ã¾ã™: "${transcript}"`);
      onChatProcessStart(transcript);
      setUserMessage('');
      console.log('âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé€ä¿¡å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ');
      return true;
    } catch (error) {
      console.error('âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé€ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:', error);
      return false;
    } finally {
      // å°‘ã—é…å»¶ã—ã¦ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆéåŒæœŸå‡¦ç†ã®ç«¶åˆã‚’é¿ã‘ã‚‹ï¼‰
      setTimeout(() => {
        isSubmittingRef.current = false;
      }, 300);
    }
  }, [onChatProcessStart]);

  // stopListeningé–¢æ•°ã®å…ˆè¡Œå®£è¨€ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯ä¸‹éƒ¨ã§è¡Œã†ï¼‰
  const stopListening = useCallback(async () => {
    if (stopListeningRef.current) {
      await stopListeningRef.current()
    }
  }, [])

  // ç„¡éŸ³æ¤œå‡ºã®ç¹°ã‚Šè¿”ã—ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†é–¢æ•°
  const startSilenceDetection = useCallback(
    (stopListeningFn: () => Promise<void>) => {
      // å‰å›ã®ã‚¿ã‚¤ãƒãƒ¼ãŒã‚ã‚Œã°è§£é™¤
      if (silenceCheckInterval.current) {
        clearInterval(silenceCheckInterval.current)
      }

      // éŸ³å£°æ¤œå‡ºæ™‚åˆ»ã‚’è¨˜éŒ²
      lastSpeechTimestamp.current = Date.now()
      
      // speechEndedRefã‚’å¿…ãšãƒªã‚»ãƒƒãƒˆ - ã“ã‚ŒãŒé‡è¦
      speechEndedRef.current = false
      console.log(
        'ğŸ¤ ç„¡éŸ³æ¤œå‡ºã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®è¨­å®šå€¤ã«åŸºã¥ã„ã¦è‡ªå‹•é€ä¿¡ã—ã¾ã™ã€‚speechEndedRef:', speechEndedRef.current
      )

      // 250msé–“éš”ã§ç„¡éŸ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
      silenceCheckInterval.current = setInterval(() => {
        // ç¾åœ¨æ™‚åˆ»ã¨æœ€çµ‚éŸ³å£°æ¤œå‡ºæ™‚åˆ»ã®å·®ã‚’è¨ˆç®—
        const silenceDuration = Date.now() - lastSpeechTimestamp.current

        // ç„¡éŸ³çŠ¶æ…‹ãŒ5ç§’ä»¥ä¸Šç¶šã„ãŸå ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšéŸ³å£°èªè­˜ã‚’åœæ­¢
        if (silenceDuration >= 5000 && !speechEndedRef.current) {
          console.log(
            `â±ï¸ ${silenceDuration}ms ã®é•·æ™‚é–“ç„¡éŸ³ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚`
          )
          speechEndedRef.current = true
          
          const trimmedTranscript = transcriptRef.current.trim();
          if (trimmedTranscript) {
            console.log(`ğŸ“ é•·æ™‚é–“ç„¡éŸ³ã§ã®èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ: "${trimmedTranscript}"`);
            sendTranscriptAsPrompt(trimmedTranscript);
          }
          
          stopListeningFn()

          // ãƒˆãƒ¼ã‚¹ãƒˆé€šçŸ¥ã‚’è¡¨ç¤º
          toastStore.getState().addToast({
            message: t('Toasts.NoSpeechDetected'),
            type: 'info',
            tag: 'no-speech-detected-long-silence',
          })
        }
        // ç„¡éŸ³çŠ¶æ…‹ãŒ2ç§’ä»¥ä¸Šç¶šã„ãŸã‹ã¤ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯è‡ªå‹•é€ä¿¡
        else if (
          settingsStore.getState().noSpeechTimeout > 0 &&
          silenceDuration >= settingsStore.getState().noSpeechTimeout * 1000 &&
          !speechEndedRef.current
        ) {
          const trimmedTranscript = transcriptRef.current.trim()
          console.log(
            `â±ï¸ ${silenceDuration}ms ã®ç„¡éŸ³ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆé–¾å€¤: ${settingsStore.getState().noSpeechTimeout * 1000}msï¼‰ã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒ0ç§’ã®å ´åˆã¯è‡ªå‹•é€ä¿¡ã¯ç„¡åŠ¹ã§ã™ã€‚`
          )
          console.log(`ğŸ“ èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ: "${trimmedTranscript}"`)

          if (
            trimmedTranscript &&
            settingsStore.getState().noSpeechTimeout > 0
          ) {
            speechEndedRef.current = true
            console.log('âœ… ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ã‚’å®Ÿè¡Œã—ã¾ã™')
            // ç„¡éŸ³æ¤œå‡ºã§è‡ªå‹•é€ä¿¡
            sendTranscriptAsPrompt(trimmedTranscript);
            stopListeningFn()
          }
        }
      }, 250) // 250msã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    },
    [sendTranscriptAsPrompt, t]
  )

  // startSilenceDetectionã‚’Refã«ä¿å­˜
  useEffect(() => {
    startSilenceDetectionRef.current = startSilenceDetection
  }, [startSilenceDetection])

  // ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–¢æ•°
  const resampleAudio = (
    audioData: Float32Array,
    fromSampleRate: number,
    toSampleRate: number
  ): Float32Array => {
    const ratio = fromSampleRate / toSampleRate
    const newLength = Math.round(audioData.length / ratio)
    const result = new Float32Array(newLength)

    for (let i = 0; i < newLength; i++) {
      const position = i * ratio
      const leftIndex = Math.floor(position)
      const rightIndex = Math.ceil(position)
      const fraction = position - leftIndex

      if (rightIndex >= audioData.length) {
        result[i] = audioData[leftIndex]
      } else {
        result[i] =
          (1 - fraction) * audioData[leftIndex] + fraction * audioData[rightIndex]
      }
    }

    return result
  }

  // ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒ¢ãƒãƒ©ãƒ«å¤‰æ›ã‚’è¡Œã†é–¢æ•°
  const processAudio = (audioBuffer: AudioBuffer): Float32Array => {
    const targetSampleRate = 24000
    const numChannels = audioBuffer.numberOfChannels

    // ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    let monoData = new Float32Array(audioBuffer.length)
    for (let i = 0; i < audioBuffer.length; i++) {
      let sum = 0
      for (let channel = 0; channel < numChannels; channel++) {
        sum += audioBuffer.getChannelData(channel)[i]
      }
      monoData[i] = sum / numChannels
    }

    // ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    return resampleAudio(monoData, audioBuffer.sampleRate, targetSampleRate)
  }

  // Float32Array ã‚’ PCM16 ArrayBuffer ã«å¤‰æ›ã™ã‚‹é–¢æ•°
  const floatTo16BitPCM = (float32Array: Float32Array) => {
    const buffer = new ArrayBuffer(float32Array.length * 2)
    const view = new DataView(buffer)
    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]))
      view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7fff, true)
    }
    return buffer
  }

  // Float32Array ã‚’ base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ PCM16 ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹é–¢æ•°
  const base64EncodeAudio = (float32Array: Float32Array) => {
    const arrayBuffer = floatTo16BitPCM(float32Array)
    let binary = ''
    const bytes = new Uint8Array(arrayBuffer)
    const chunkSize = 0x8000 // 32KB chunk size
    for (let i = 0; i < bytes.length; i += chunkSize) {
      binary += String.fromCharCode.apply(
        null,
        Array.from(bytes.subarray(i, i + chunkSize))
      )
    }
    return btoa(binary)
  }

  // sendAudioBufferé–¢æ•°ã‚’ã“ã“ã«ç§»å‹•
  const sendAudioBuffer = useCallback(() => {
    if (audioBufferRef.current && audioBufferRef.current.length > 0) {
      const base64Chunk = base64EncodeAudio(audioBufferRef.current)
      const ss = settingsStore.getState()
      const wsManager = webSocketStore.getState().wsManager
      if (wsManager?.websocket?.readyState === WebSocket.OPEN) {
        let sendContent: { type: string; text?: string; audio?: string }[] = []

        if (ss.realtimeAPIModeContentType === 'input_audio') {
          console.log('Sending buffer. Length:', audioBufferRef.current.length)
          sendContent = [
            {
              type: 'input_audio',
              audio: base64Chunk,
            },
          ]
        } else {
          const currentText = transcriptRef.current.trim()
          console.log('Sending text. userMessage:', currentText)
          if (currentText) {
            sendContent = [
              {
                type: 'input_text',
                text: currentText,
              },
            ]
          }
        }

        if (sendContent.length > 0) {
          wsManager.websocket.send(
            JSON.stringify({
              type: 'conversation.item.create',
              item: {
                type: 'message',
                role: 'user',
                content: sendContent,
              },
            })
          )
          wsManager.websocket.send(
            JSON.stringify({
              type: 'response.create',
            })
          )
        }
      }
      audioBufferRef.current = null // é€ä¿¡å¾Œã«ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
    } else {
      console.error('éŸ³å£°ãƒãƒƒãƒ•ã‚¡ãŒç©ºã§ã™')
    }
  }, [])

  // sendAudioBufferã‚’Refã«ä¿å­˜
  useEffect(() => {
    sendAudioBufferRef.current = sendAudioBuffer
  }, [sendAudioBuffer])

  // SpeechRecognitionã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
  const recreateRecognition = useCallback(() => {
    console.log('ğŸ”„ SpeechRecognitionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†ä½œæˆã—ã¾ã™');
    
    // VADã®çŠ¶æ…‹ã‚‚ãƒªã‚»ãƒƒãƒˆ - ä¿®æ­£ç‚¹1
    resetVadBetweenSessions();
    
    // ç¾åœ¨ã®SpeechRecognitionã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (recognition) {
      try {
        recognition.onstart = null;
        recognition.onresult = null;
        recognition.onerror = null;
        recognition.onend = null;
        recognition.onspeechstart = null;
        recognition.onspeechend = null;
        recognition.abort();
      } catch (e) {
        console.log('æ—¢å­˜ã®recognitionã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼:', e);
      }
    }
    
    // æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (SpeechRecognition) {
      const newRecognition = new SpeechRecognition();
      newRecognition.lang = getVoiceLanguageCode(selectLanguage);
      newRecognition.continuous = true;
      newRecognition.interimResults = true;

      // éŸ³å£°èªè­˜é–‹å§‹æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ 
      newRecognition.onstart = () => {
        console.log('ğŸ™ï¸ Speech recognition started - æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³');
        // éŸ³å£°èªè­˜é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        recognitionStartTimeRef.current = Date.now();
        // éŸ³å£°æ¤œå‡ºãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        speechDetectedRef.current = false;
        // å¿…ãšé€ä¿¡æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        speechEndedRef.current = false;

        // 5ç§’å¾Œã«éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š
        initialSpeechCheckTimerRef.current = setTimeout(() => {
          // éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯éŸ³å£°èªè­˜ã‚’åœæ­¢
          if (!speechDetectedRef.current && isListeningRef.current) {
            console.log(
              'â±ï¸ 5ç§’é–“éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚'
            );
            stopListening();

            // å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ¼ã‚¹ãƒˆé€šçŸ¥ã‚’è¡¨ç¤º
            toastStore.getState().addToast({
              message: t('Toasts.NoSpeechDetected'),
              type: 'info',
              tag: 'no-speech-detected',
            });
          }
        }, INITIAL_SPEECH_TIMEOUT);

        // ç„¡éŸ³æ¤œå‡ºã‚’é–‹å§‹
        if (stopListeningRef.current && startSilenceDetectionRef.current) {
          startSilenceDetectionRef.current(stopListeningRef.current);
        }
      };

      // éŸ³å£°å…¥åŠ›æ¤œå‡ºæ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ 
      newRecognition.onspeechstart = () => {
        console.log('ğŸ—£ï¸ éŸ³å£°å…¥åŠ›ã‚’æ¤œå‡ºã—ã¾ã—ãŸ');
        // éŸ³å£°æ¤œå‡ºãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
        speechDetectedRef.current = true;
        // éŸ³å£°æ¤œå‡ºæ™‚åˆ»ã‚’æ›´æ–°
        lastSpeechTimestamp.current = Date.now();
      };

      // çµæœãŒè¿”ã£ã¦ããŸæ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ï¼ˆéŸ³å£°æ¤œå‡ºä¸­ï¼‰
      newRecognition.onresult = (event) => {
        if (!isListeningRef.current) return;

        // éŸ³å£°ã‚’æ¤œå‡ºã—ãŸã®ã§ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°
        lastSpeechTimestamp.current = Date.now();
        // éŸ³å£°æ¤œå‡ºãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹ï¼ˆçµæœãŒè¿”ã£ã¦ããŸã¨ã„ã†ã“ã¨ã¯éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ï¼‰
        speechDetectedRef.current = true;

        const transcript = Array.from(event.results)
          .map((result) => result[0].transcript)
          .join('');
        console.log('ğŸ”¤ èªè­˜ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°:', transcript);
        transcriptRef.current = transcript;
        setUserMessage(transcript);
      };

      // éŸ³å£°å…¥åŠ›çµ‚äº†æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©
      newRecognition.onspeechend = () => {
        console.log('ğŸ›‘ éŸ³å£°å…¥åŠ›ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ãŒå‹•ä½œä¸­ã§ã™ã€‚');
        // éŸ³å£°å…¥åŠ›ãŒçµ‚ã‚ã£ãŸãŒã€ç„¡éŸ³æ¤œå‡ºã¯ãã®ã¾ã¾ç¶™ç¶šã™ã‚‹
        // ã‚¿ã‚¤ãƒãƒ¼ãŒ2ç§’å¾Œã«å‡¦ç†ã™ã‚‹
      };

      // éŸ³å£°èªè­˜çµ‚äº†æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©
      newRecognition.onend = () => {
        console.log('ğŸ”š Recognition ended - ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†');
        // ç„¡éŸ³æ¤œå‡ºã‚’ã‚¯ãƒªã‚¢
        if (clearSilenceDetectionRef.current) {
          clearSilenceDetectionRef.current();
        }
        // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
        clearInitialSpeechCheckTimer();
      };

      newRecognition.onerror = (event) => {
        console.error('ğŸš¨ Speech recognition error:', event.error);
        if (clearSilenceDetectionRef.current) {
          clearSilenceDetectionRef.current();
        }
        // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
        clearInitialSpeechCheckTimer();
        stopListening();
      };

      setRecognition(newRecognition);
      return newRecognition;
    }
    
    return null;
  }, [selectLanguage, getVoiceLanguageCode, clearInitialSpeechCheckTimer, stopListening, t]);

  // ã“ã“ã§æœ€çµ‚çš„ãªstopListeningå®Ÿè£…ã‚’è¡Œã†
  const stopListeningImpl = useCallback(async () => {
    console.log('ğŸ›‘ stopListeningImpl: éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™ã€‚speechEndedRef:', speechEndedRef.current);
    
    // çŠ¶æ…‹ã‚’ãƒ­ã‚°å‡ºåŠ›
    const currentState = {
      isListening: isListeningRef.current,
      speechDetected: speechDetectedRef.current,
      speechEnded: speechEndedRef.current,
      audioLevelAboveThreshold: audioLevelAboveThresholdRef.current,
      transcript: transcriptRef.current.trim()
    };
    console.log('Current state before stopping:', currentState);
    
    // ç„¡éŸ³æ¤œå‡ºã‚’ã‚¯ãƒªã‚¢
    if (clearSilenceDetectionRef.current) {
      clearSilenceDetectionRef.current()
    }

    // åˆæœŸéŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    clearInitialSpeechCheckTimer()

    // éŸ³å£°èªè­˜ã‚’åœæ­¢
    isListeningRef.current = false
    setIsListening(false)
    
    // éŸ³å£°æ¤œå‡ºé–¢é€£ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆspeechEndedRefã¯ã“ã“ã§ãƒªã‚»ãƒƒãƒˆã—ãªã„ - é€ä¿¡å‡¦ç†å¾Œã«ãƒªã‚»ãƒƒãƒˆï¼‰
    speechDetectedRef.current = false
    audioLevelAboveThresholdRef.current = false
    
    // VADã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ - ä¿®æ­£ç‚¹2
    cleanupVadState();
    
    if (recognition) {
      try {
        recognition.stop()
        console.log('âœ… recognition.stop()ã‚’å‘¼ã³å‡ºã—ã¾ã—ãŸ');
      } catch (error) {
        console.error('âŒ Error stopping recognition:', error);
      }

      if (realtimeAPIMode) {
        if (mediaRecorder) {
          mediaRecorder.stop()
          mediaRecorder.ondataavailable = null
          await new Promise<void>((resolve) => {
            mediaRecorder.onstop = async () => {
              console.log('stop MediaRecorder')
              if (audioChunksRef.current.length > 0) {
                const audioBlob = new Blob(audioChunksRef.current, {
                  type: 'audio/webm',
                })
                const arrayBuffer = await audioBlob.arrayBuffer()
                const audioBuffer =
                  await audioContext!.decodeAudioData(arrayBuffer)
                const processedData = processAudio(audioBuffer)

                audioBufferRef.current = processedData
                resolve()
              } else {
                console.error('éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã§ã™')
                resolve()
              }
            }
          })
        }
        // sendAudioBufferã®ä»£ã‚ã‚Šã«sendAudioBufferRef.currentã‚’ä½¿ç”¨
        if (sendAudioBufferRef.current) {
          sendAudioBufferRef.current()
        }
      }

      const trimmedTranscriptRef = transcriptRef.current.trim()
      if (isKeyboardTriggered.current) {
        const pressDuration = Date.now() - (keyPressStartTime.current || 0)
        // æŠ¼ã—ã¦ã‹ã‚‰1ç§’ä»¥ä¸Š ã‹ã¤ æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡
        // ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡ãŒæ—¢ã«è¡Œã‚ã‚Œã¦ã„ãªã„å ´åˆã®ã¿é€ä¿¡ã™ã‚‹
        if (
          pressDuration >= 1000 &&
          trimmedTranscriptRef &&
          !speechEndedRef.current
        ) {
          sendTranscriptAsPrompt(trimmedTranscriptRef);
        }
        isKeyboardTriggered.current = false
      }
    }
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†ã¨ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡å¾Œã€æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æº–å‚™
    setTimeout(() => {
      console.log('ğŸ”„ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†å¾Œã«éŸ³å£°èªè­˜ã‚’å†åˆæœŸåŒ–ã—ã¾ã™ã€‚çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆå‰speechEndedRef:', speechEndedRef.current);
      // é€ä¿¡å‡¦ç†ãŒå®Œäº†ã—ãŸå¾Œã«speechEndedRefã‚’ãƒªã‚»ãƒƒãƒˆ
      speechEndedRef.current = false;
      
      // éŸ³å£°èªè­˜ã‚’ãƒªã‚»ãƒƒãƒˆ
      recreateRecognition();
      
      // ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚¯ãƒªã‚¢
      transcriptRef.current = '';
      console.log('âœ… å…¨ã¦ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æº–å‚™å®Œäº†ã€‚');
    }, 500);
  }, [
    recognition,
    realtimeAPIMode,
    mediaRecorder,
    audioContext,
    sendTranscriptAsPrompt,
    clearInitialSpeechCheckTimer,
    recreateRecognition
  ])

  // stopListeningã®å®Ÿè£…ã‚’ä¸Šæ›¸ã
  useEffect(() => {
    stopListeningRef.current = stopListeningImpl
  }, [stopListeningImpl])
  
  // ã‚ã‚‰ã‹ã˜ã‚startListeningã®å®šç¾©ã‚’è¡Œã†
  const startListening = useCallback(async () => {
    const hasPermission = await checkMicrophonePermission();
    if (!hasPermission) return;

    // VADã®çŠ¶æ…‹ã‚’å®Œå…¨ã«ãƒªã‚»ãƒƒãƒˆ - ä¿®æ­£ç‚¹3 ã“ã‚ŒãŒäºŒè¨€ç›®ãƒ»ä¸‰è¨€ç›®ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹
    resetVadBetweenSessions();
    console.log('ğŸ‘‚ startListening: éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ - VADã‚’ãƒªã‚»ãƒƒãƒˆå®Œäº†');
    
    // æ–°ã—ã„SpeechRecognitionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    const currentRecognition = recreateRecognition() || recognition;
    
    if (currentRecognition && !isListeningRef.current && audioContext) {
      transcriptRef.current = '';
      setUserMessage('');
      try {
        console.log('ğŸ¬ recognition.start()ã‚’å‘¼ã³å‡ºã—ã¾ã™');
        currentRecognition.start();
        console.log('âœ… recognition.start()ã®å‘¼ã³å‡ºã—ã«æˆåŠŸã—ã¾ã—ãŸ');
      } catch (error) {
        console.error('âŒ Error starting recognition:', error);
        // éŸ³å£°èªè­˜ã®é–‹å§‹ã«å¤±æ•—ã—ãŸã‚‰ã€çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†è©¦è¡Œã—ãªã„
        return;
      }
      
      isListeningRef.current = true;
      setIsListening(true
