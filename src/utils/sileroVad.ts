import * as onnx from 'onnxruntime-web';

// ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
const MODEL_PATH = '/models/silero_vad.onnx';

// VADãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
let vadModel: onnx.InferenceSession | null = null;

// çŠ¶æ…‹ç®¡ç†ç”¨ã®å¤‰æ•°
let previousSpeechProb = 0;
const SPEECH_HISTORY_SIZE = 30; // ç´„1ç§’ï¼ˆ30ãƒ•ãƒ¬ãƒ¼ãƒ  Ã— 30msï¼‰
const speechProbHistory: number[] = Array(SPEECH_HISTORY_SIZE).fill(0);

// ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆç”¨ï¼‰
let currentSessionId = 0;

// ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
export async function initVadModel(): Promise<onnx.InferenceSession> {
  if (vadModel) return vadModel;

  try {
    console.log('Initializing Silero VAD model...');
    
    // ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š
    const options: onnx.InferenceSession.SessionOptions = {
      executionProviders: ['wasm'],
      graphOptimizationLevel: 'all'
    };
    
    // ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    // ã‚µãƒ¼ãƒãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    const modelPath = `${MODEL_PATH}?t=${Date.now()}`;
    console.log(`Loading model from: ${modelPath}`);
    
    try {
      vadModel = await onnx.InferenceSession.create(modelPath, options);
      console.log('Silero VAD model loaded successfully');
    } catch (innerError) {
      console.warn('Failed to load model with timestamp, trying original path:', innerError);
      // ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ‘ã‚¹ã§å†è©¦è¡Œ
      vadModel = await onnx.InferenceSession.create(MODEL_PATH, options);
      console.log('Silero VAD model loaded successfully (original path)');
    }
    
    return vadModel;
  } catch (error) {
    console.error('Error initializing Silero VAD model:', error);
    // ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼ã™ã‚‹ãŒã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã¯é¿ã‘ã‚‹
    console.warn('Continuing without VAD model, will use fallback methods');
    return null as any; // ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ä»£ã‚ã‚Šã« null ã‚’è¿”ã™
  }
}

// éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
export function resampleTo16kHz(audioData: Float32Array, originalSampleRate: number): Float32Array {
  if (originalSampleRate === 16000) return audioData;
  
  const ratio = originalSampleRate / 16000;
  const newLength = Math.round(audioData.length / ratio);
  const result = new Float32Array(newLength);
  
  for (let i = 0; i < newLength; i++) {
    const originalIndex = Math.min(Math.floor(i * ratio), audioData.length - 1);
    result[i] = audioData[originalIndex];
  }
  
  return result;
}

// éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ­£è¦åŒ–
function normalizeFrame(frame: Float32Array): Float32Array {
  const max = Math.max(...frame.map(Math.abs));
  if (max === 0) return frame;
  
  return frame.map(x => x / max);
}

// ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
function frameToTensor(frame: Float32Array): onnx.Tensor {
  return new onnx.Tensor('float32', frame, [1, frame.length]);
}

// éŸ³å£°æ¤œå‡ºã®ç¢ºç‡ã‚’è¨ˆç®—
export async function detectSpeechProb(
  audioData: Float32Array,
  sampleRate: number = 48000
): Promise<number> {
  if (!vadModel) {
    try {
      await initVadModel();
    } catch (error) {
      console.error('Failed to initialize VAD model:', error);
      return 0;
    }
  }
  
  try {
    // 16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    const resampledData = resampleTo16kHz(audioData, sampleRate);
    
    // ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ­£è¦åŒ–
    const normalizedFrame = normalizeFrame(resampledData);
    
    // å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ä½œæˆ
    const inputTensor = frameToTensor(normalizedFrame);
    
    // ãƒ¢ãƒ‡ãƒ«æ¨è«–ã®å®Ÿè¡Œ
    const feeds: Record<string, onnx.Tensor> = { input: inputTensor };
    const results = await vadModel!.run(feeds);
    
    // çµæœã®å–å¾—ï¼ˆç™ºè©±ç¢ºç‡ï¼‰
    const outputData = results.output.data as Float32Array;
    const speechProb = outputData[0];
    
    // å±¥æ­´æ›´æ–°
    speechProbHistory.shift();
    speechProbHistory.push(speechProb);
    
    // å¹³æ»‘åŒ–ã•ã‚ŒãŸç¢ºç‡ã‚’è¿”ã™
    const smoothedProb = smoothProbability(speechProb);
    previousSpeechProb = smoothedProb;
    
    return smoothedProb;
  } catch (error) {
    console.error('Error during speech detection:', error);
    return previousSpeechProb;
  }
}

// ç¢ºç‡ã®å¹³æ»‘åŒ–
function smoothProbability(currentProb: number): number {
  // æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨ã—ãŸå¹³æ»‘åŒ–
  const alpha = 0.3; // å¹³æ»‘åŒ–ä¿‚æ•°ï¼ˆ0.0-1.0ï¼‰
  return alpha * currentProb + (1 - alpha) * previousSpeechProb;
}

// å¹³å‡éŸ³å£°ç¢ºç‡ã®è¨ˆç®—
export function getAverageSpeechProb(): number {
  return speechProbHistory.reduce((sum, prob) => sum + prob, 0) / SPEECH_HISTORY_SIZE;
}

// éŸ³å£°çŠ¶æ…‹ã®åˆ¤å®šï¼ˆè©±ã—ã¦ã„ã‚‹/è©±ã—ã¦ã„ãªã„ï¼‰
export function isSpeaking(
  currentProb: number, 
  speechThreshold: number = 0.5, 
  silenceThreshold: number = 0.3
): boolean {
  // ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ã—ãã„å€¤ã‚’ä½¿ç”¨
  if (previousSpeechProb >= speechThreshold) {
    return currentProb >= silenceThreshold; // æ—¢ã«è©±ã—ã¦ã„ã‚‹å ´åˆã¯ã€ã‚ˆã‚Šä½ã„ã—ãã„å€¤ã‚’ä½¿ç”¨
  } else {
    return currentProb >= speechThreshold; // è©±ã—ã¦ã„ãªã„å ´åˆã¯ã€é«˜ã„ã—ãã„å€¤ã‚’è¦æ±‚
  }
}

// VADã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹é–¢æ•°
export function resetVadState(): void {
  console.log('ğŸ”„ VADã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™');
  previousSpeechProb = 0;
  for (let i = 0; i < SPEECH_HISTORY_SIZE; i++) {
    speechProbHistory[i] = 0;
  }
  currentSessionId++;
  console.log(`ğŸ†” æ–°ã—ã„VADã‚»ãƒƒã‚·ãƒ§ãƒ³ID: ${currentSessionId}`);
}

// ç™ºè©±ã®å®‰å®šæ€§ã‚’è©•ä¾¡
export function getSpeechStability(): number {
  const recentHistory = speechProbHistory.slice(-10);
  const mean = recentHistory.reduce((sum, val) => sum + val, 0) / recentHistory.length;
  
  // åˆ†æ•£ã®è¨ˆç®—
  const variance = recentHistory.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / recentHistory.length;
  
  // å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€1ãŒæœ€ã‚‚å®‰å®šï¼‰
  return Math.max(0, 1 - Math.sqrt(variance) * 2);
}
