import { Talk } from './messages'
import homeStore from '@/features/stores/home'
import settingsStore from '@/features/stores/settings'

export class Live2DHandler {
  private static idleMotionInterval: NodeJS.Timeout | null = null // ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«IDã‚’ä¿æŒ
  private static currentAudio: HTMLAudioElement | null = null // ç¾åœ¨å†ç”Ÿä¸­ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª

  /**
   * ç¾åœ¨å†ç”Ÿä¸­ã®éŸ³å£°ã‚’ä¸­æ–­ã™ã‚‹
   */
  static interruptSpeaking(): void {
    if (Live2DHandler.currentAudio) {
      try {
        console.log('ğŸ”‡ Live2DHandler: ç¾åœ¨å†ç”Ÿä¸­ã®éŸ³å£°ã‚’åœæ­¢ã—ã¾ã™');
        Live2DHandler.currentAudio.pause();
        Live2DHandler.currentAudio.currentTime = 0;
        Live2DHandler.currentAudio = null;
      } catch (e) {
        console.error('Live2DéŸ³å£°åœæ­¢å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:', e);
      }
    }
  }

  static async speak(
    audioBuffer: ArrayBuffer,
    talk: Talk,
    isNeedDecode: boolean = true
  ) {
    const hs = homeStore.getState()
    const ss = settingsStore.getState()
    const live2dViewer = hs.live2dViewer
    if (!live2dViewer) return

    let expression: string | undefined
    let motion: string | undefined
    switch (talk.emotion) {
      case 'neutral':
        expression =
          ss.neutralEmotions[
            Math.floor(Math.random() * ss.neutralEmotions.length)
          ]
        motion = ss.neutralMotionGroup
        break
      case 'happy':
        expression =
          ss.happyEmotions[Math.floor(Math.random() * ss.happyEmotions.length)]
        motion = ss.happyMotionGroup
        break
      case 'sad':
        expression =
          ss.sadEmotions[Math.floor(Math.random() * ss.sadEmotions.length)]
        motion = ss.sadMotionGroup
        break
      case 'angry':
        expression =
          ss.angryEmotions[Math.floor(Math.random() * ss.angryEmotions.length)]
        motion = ss.angryMotionGroup
        break
      case 'relaxed':
        expression =
          ss.relaxedEmotions[
            Math.floor(Math.random() * ss.relaxedEmotions.length)
          ]
        motion = ss.relaxedMotionGroup
        break
    }

    // AudioContextã®ä½œæˆ
    const audioContext = new AudioContext()
    let decodedAudio: AudioBuffer

    if (isNeedDecode) {
      // åœ§ç¸®éŸ³å£°ã®å ´åˆ
      decodedAudio = await audioContext.decodeAudioData(audioBuffer)
    } else {
      // PCM16å½¢å¼ã®å ´åˆ
      const pcmData = new Int16Array(audioBuffer)
      const floatData = new Float32Array(pcmData.length)
      for (let i = 0; i < pcmData.length; i++) {
        floatData[i] =
          pcmData[i] < 0 ? pcmData[i] / 32768.0 : pcmData[i] / 32767.0
      }
      decodedAudio = audioContext.createBuffer(1, floatData.length, 24000) // sampleRateã¯å¿…è¦ã«å¿œã˜ã¦èª¿æ•´
      decodedAudio.getChannelData(0).set(floatData)
    }

    // ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Blobã«å¤‰æ›
    const offlineContext = new OfflineAudioContext(
      decodedAudio.numberOfChannels,
      decodedAudio.length,
      decodedAudio.sampleRate
    )
    const source = offlineContext.createBufferSource()
    source.buffer = decodedAudio
    source.connect(offlineContext.destination)
    source.start()

    const renderedBuffer = await offlineContext.startRendering()
    const audioBlob = await new Blob([this.audioBufferToWav(renderedBuffer)], {
      type: 'audio/wav',
    })
    const audioUrl = URL.createObjectURL(audioBlob)

    // Live2Dãƒ¢ãƒ‡ãƒ«ã®è¡¨æƒ…ã‚’è¨­å®š
    if (expression) {
      live2dViewer.expression(expression)
    }
    if (motion) {
      Live2DHandler.stopIdleMotion()
      live2dViewer.motion(motion, undefined, 3)
    }

    // éŸ³å£°å†ç”Ÿã®å®Œäº†ã‚’å¾…ã¤
    // live2dViewer.speakã§ã¯éŸ³å£°å®Œäº†ã‚’æ¤œçŸ¥ã§ããªã„ã®ã§ã€Audioã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦éŸ³å£°å†ç”Ÿå®Œäº†ã‚’æ¤œçŸ¥ã—ã¦ã„ã‚‹
    // Audioã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ–¹ã‚‚å†ç”Ÿã™ã‚‹ã¨äºŒé‡ã«èã“ãˆã¦ã—ã¾ã†ã®ã§ã€å†ç”ŸéŸ³é‡ã‚’æœ€ä½é™ã«è¨­å®š
    // TODO: ã‚‚ã£ã¨ã„ã„æ–¹æ³•ãŒã‚ã‚Œã°ãã‚Œã«å¤‰æ›´ã™ã‚‹
    await new Promise<void>((resolve) => {
      // æ—¢å­˜ã®éŸ³å£°ã‚’åœæ­¢
      Live2DHandler.interruptSpeaking();
      
      // æ–°ã—ã„éŸ³å£°ã‚’è¨­å®š
      const audio = new Audio(audioUrl);
      audio.volume = 0.01;
      audio.onended = () => {
        Live2DHandler.currentAudio = null;
        resolve();
        URL.revokeObjectURL(audioUrl);
      };
      
      // ç¾åœ¨ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã¨ã—ã¦ç™»éŒ²
      Live2DHandler.currentAudio = audio;
      
      // å†ç”Ÿé–‹å§‹
      audio.play();
      live2dViewer.speak(audioUrl);
    })
  }

  static async resetToIdle() {
    // ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚’åœæ­¢
    Live2DHandler.stopIdleMotion()

    const hs = homeStore.getState()
    const ss = settingsStore.getState()
    const live2dViewer = hs.live2dViewer
    if (!live2dViewer) return

    // Live2Dãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã®å ´åˆã¯æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
    if (ss.modelType !== 'live2d') return

    const idleMotion = ss.idleMotionGroup || 'Idle'
    live2dViewer.motion(idleMotion)
    const expression =
      ss.neutralEmotions[Math.floor(Math.random() * ss.neutralEmotions.length)]
    if (expression) {
      live2dViewer.expression(expression)
    }

    // 5ç§’ã”ã¨ã®ã‚¢ã‚¤ãƒ‰ãƒ«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å†ç”Ÿã‚’é–‹å§‹
    Live2DHandler.startIdleMotion(idleMotion, live2dViewer)
  }

  // ã‚¢ã‚¤ãƒ‰ãƒ«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«é–‹å§‹
  private static startIdleMotion(idleMotion: string, live2dViewer: any) {
    const ss = settingsStore.getState()
    if (ss.modelType !== 'live2d') return

    this.idleMotionInterval = setInterval(() => {
      const currentSs = settingsStore.getState()
      if (currentSs.modelType !== 'live2d') {
        this.stopIdleMotion()
        return
      }
      live2dViewer.motion(idleMotion)
    }, 5000)
  }

  // ã‚¢ã‚¤ãƒ‰ãƒ«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«åœæ­¢
  private static stopIdleMotion() {
    if (this.idleMotionInterval) {
      clearInterval(this.idleMotionInterval)
      this.idleMotionInterval = null
    }
  }

  // WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
  private static audioBufferToWav(buffer: AudioBuffer): ArrayBuffer {
    const numOfChan = buffer.numberOfChannels
    const length = buffer.length * numOfChan * 2
    const buffer2 = new ArrayBuffer(44 + length)
    const view = new DataView(buffer2)
    const channels = []
    let sample
    let offset = 0
    let pos = 0

    // WAVãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½œæˆ
    setUint32(0x46464952) // "RIFF"
    setUint32(36 + length) // file length
    setUint32(0x45564157) // "WAVE"
    setUint32(0x20746d66) // "fmt "
    setUint32(16) // section length
    setUint16(1) // PCM
    setUint16(numOfChan)
    setUint32(buffer.sampleRate)
    setUint32(buffer.sampleRate * 2 * numOfChan) // byte rate
    setUint16(numOfChan * 2) // block align
    setUint16(16) // bits per sample
    setUint32(0x61746164) // "data"
    setUint32(length)

    // ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    for (let i = 0; i < buffer.numberOfChannels; i++) {
      channels.push(buffer.getChannelData(i))
    }

    // ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–
    while (pos < buffer.length) {
      for (let i = 0; i < numOfChan; i++) {
        sample = Math.max(-1, Math.min(1, channels[i][pos]))
        sample = (0.5 + sample < 0 ? sample * 32768 : sample * 32767) | 0
        view.setInt16(44 + offset, sample, true)
        offset += 2
      }
      pos++
    }

    function setUint16(data: number) {
      view.setUint16(pos, data, true)
      pos += 2
    }

    function setUint32(data: number) {
      view.setUint32(pos, data, true)
      pos += 4
    }

    return buffer2
  }
}
